我们在之前的章节中介绍了 ChatGPT 的模型建模、模型结构、工作机制。除此之外，恰当的模型训练方式对其最终取得的效果也至关重要。

NLP 领域模型训练策略的改变总共经历了四个阶段，这也是 GPT 模型的训练方式进化史。


![9-1.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/40d6ab4a89114c6986302f57bf6ba2a5~tplv-k3u1fbpfcp-watermark.image?)

ChatGPT 的模型训练方式依然汲取了大规模语言模型 （LM） 预训练，以及小样本学习的思想。因此，本节我们先来重点梳理一下前三个阶段。

  


# 纯监督学习

这种方式是最早期的 NLP 模型训练方式，也是最传统的机器学习建模方式。这种模型训练方式主要还是针对特定的 NLP 任务来完成的，诸如文本分类、实体识别、文本摘要抽取、机器翻译等。

  


> 为了说明监督学习的数据组织模式，我们准备了 3 条标注样例如下：
>
> -   文本分类：
>
> 文本：jionlp 开发工具包确实挺好用的，非常感谢博主的分享~~~~
>
> 类别：正面
>
> -   实体识别：就是指从自然语言文本中，抽取出如公司名、人名、药物名称、时间、地名等具有特定意义的词汇或短语，这些内容有特定的用途。一般需要标定实体的文本、类型和在文本中的位置。
>
> 文本：派飞公司的业务这两年越做越大，快要上市了。
>
> 标记：['text': '派飞', 'type': 'company', 'offset': [0, 2]]
>
> -   机器翻译：
>
> 文本：今天中午到明天早上，我市将有中到大雨，请市民出行做好防雨准备。
>
> 译文：From noon today to tomorrow morning, there will be moderate to heavy rain in our city. Citizens are advised to prepare for rain when going out.

除此之外，NLP 领域还包括分词、语义关系抽取、实体消歧等等特定领域的任务，每个任务都可以按上述方式构建标注数据集，其一般形式为：【文本，标注信息】。

  


纯监督学习的模型训练方式，就是第 8 节中我们所介绍的内容，这种训练方式存在很多难以克服的缺点：

-   **极度依赖标注数据集**：AI 领域有一句广为流传的真理——有多少人工，就有多少智能。其含义就是指，模型通过学习标注数据获得智能，而标注数据全靠人力一点点完成。而人工筹措如此多的数据集是极为耗时耗力的，也严重制约了 AI 技术的发展。即便是 OpenAI，也雇佣了大量的全世界各地区的廉价劳动力做数据标注，但人力的效率远远满足不了模型规模的扩张。

-   **模型只针对特定任务**：早期的 AI 模型都只针对特定任务，而不存在一个模型完成多种任务的方式存在。这导致了模型只局限于一个非常小的规模，一种模型只能完成一种任务，局限性太强。

-   **模型泛化** **能力** **差**。

所谓模型的**泛化能力**，就是模型在处理未曾见过的数据时的表现能力。在机器学习中，我们通常会将数据集分为训练集和测试集，模型在训练集上进行训练，然后在测试集上进行测试，以评估模型的性能。训练集就是模型见过的数据，而测试集则是模型未见过的。如果模型在训练集上表现良好，但在测试集上表现不佳，那就说明模型的泛化能力不足。

  


例如，我们教会了模型计算“3+2=5”、“5+8=13”、但是如果向模型提问“9+7=？”，模型却不能正确回答，就说明模型泛化能力差，模型不具备**举一反三**的能力。

  


# 预训练+微调（Finetune）

预训练+ 微调（finetune）是一种在 NLP 领域中广泛使用的技术。它的基本思想是，使用大量的未标记数据进行预训练，然后使用少量的标注数据进行微调（finetune），从而适应特定的 NLP 任务。直到 ChatGPT 出现之前，这是绝大多数 NLP 模型任务的基本工作流程。

  


模型预训练就是构造一个语言模型，利用互联网上大规模的未经人工标注的文本进行上下文联系学习。我们在第 3、4、5、6 节中介绍了 ChatGPT 的语言模型结构，这整个模型就是在完成预训练工作。

  


而在预训练阶段完成之后，GPT 初代就采用微调（finetune）的方式对很多下游任务做监督学习。所谓微调，本质和纯监督学习的操作过程完全一样，唯一的区别在于模型以梯度下降法训练的过程中，参数调整的幅度和范围比较微小。

-   纯监督学习的方式，是指模型在一个**完全随机初始化的参数**基础上进行训练；例如，在第 8 节中，猫狗分类模型的所有权重值是我一开始随机想出来的，随机设定的。
-   预训练+finetune 的方式，是模型在一个**已经具备一定语言知识和语言能力（经过了预训练）的参数**基础上进行训练。即在微调前，我们已经得到了一个预先训练好的参数集。

  


这种方法相比纯监督学习有一定好处：同样是训练一个针对特定任务的 NLP 模型，有预训练的模型提前已经具备了一定的语言基础，学习特定的任务速度更快，效果更好，同时需要的标注数据量级相对较少，模型泛化效果好。

  


> 若把训练 AI 模型比作教育小孩的话，假设现在有一个 1 岁还不会说话的幼儿，以及一个 5 岁孩子，两个人都从来没接触过加法算术。我们教 5 岁孩子算加法肯定要比教一岁幼儿更容易。原因在于，即便 5 岁孩子从来没有接触过数学，但是他已经在过去 5 年的成长中听到、看到了大量的信息，这些信息有助于他理解数学。
>
> 在这个例子中，学算术类似于学习某个特定 NLP 任务，小孩成长过程中接收到大量的信息，类比于 NLP 模型的预训练。教 5 岁孩子学算术，就类似于**预训练+fintune**，而教 1 岁幼儿算术，则是原始的纯监督学习。

  


# **In-context learning**

## In-context learning 原理

为了克服模型训练需要依赖大量数据的局限性，GPT-3 提出了一种叫做 in-context 的学习方式。直译过来，就是“在上下文中学习”，用中文来解释的话，就是让 GPT 模型在上下文中学习要学的任务和内容，下面举一个例子进行解释。我们知道，GPT 系列模型的建模方式如下：


![9-2.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ddebf866a006421c9e573fbb29b541a4~tplv-k3u1fbpfcp-watermark.image?)

  


> 用户输入到 GPT-3：**你觉得 JioNLP 是个好用的工具吗？**
>
> GPT-3输出1：**我觉得很好啊。**
>
> GPT-3输出2：**JioNLP是什么东西？**
>
> GPT-3输出3：**你饿不饿，我给你做碗面吃……**
>
> GPT-3输出4：**Do you think jionlp is a good tool?**

  


按理来讲，针对**机器翻译**任务，我们希望模型输出第 4 句，而针对**对话**任务，我们希望模型能够输出第 1、2 句中的任何一句。另外，第 3 句这个输出句子前言不搭后语，不是恰当的输出语句。

根据第 3 节语言模型的介绍，ChatGPT 模型的输入实际上就是一段 context 上下文。这时就有了 in-context 学习，也就是我们对模型进行引导（或提示，prompt），教会它应当输出什么内容。如果我们希望它输出翻译内容，那么，应该给模型如下输入：

> 用户输入到 GPT-3：**请把以下中文翻译成英文：** 你觉得 JioNLP 是个好用的工具吗？

如果想让模型回答问题：

> 用户输入到 GPT-3：**模型模型你给我说说，** 你觉得 JioNLP 是个好用的工具吗？

我们将上述的模式进行一般化建模：$$p(output|input, task)$$。

其中，task 就是在原输入基础上，补充指定模型完成的任务。即在对语言模型建模时，模型的输入既要指明文本输入内容，又要指明让模型完成什么任务。OK，这样模型就可以根据用户提示的情境做针对性的回答了。

  


## In-context learning 的训练

In-context learning 的模型训练方式与之前所属的语言模型预训练流程完全一致，但其中已经构建了大量的各种各样的任务，如下图所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9aa23df9459249069ea8adfc93c70334~tplv-k3u1fbpfcp-zoom-1.image)

整个学习过程依然是基于随机梯度下降（SGD）方法的预训练，但是在数据组织上添加了数学计算，把乱序的单词恢复原序（unscrambling words）、英文到法文翻译。这样，模型就学会了处理多种任务，即 in-context learning。

  


## zero-shot、one-shot、few-shot 小样本学习

我们在模型预训练阶段，是不可能穷尽所有学习任务 task 的。因此，对于 ChatGPT 的预训练模型来说，一定存在大量模型没见过的学习任务。

  


> 例如，模型中可能缺少如下任务：
>
> -   她对于他的求爱，感到十分感动，但是拒绝了他。=> 十动然拒
> -   他完全不知道发生了什么事情，但是觉得很厉害。=> 不明觉厉
> -   人生已经如此艰难，有些事情就不要拆穿了。=> ?
>
> 对于人类的大脑来说，可能从来没见过这种语言任务，但是通过上述两个例子就能明白，这需要把一句话凝聚成一个类似成语的四字短语。

  


对于语言模型来说，它也需要能够根据若干例子，明白用户想要它做的任务，尽管这种任务在预训练数据集里从来没出现过。在 in-context learning 过程中，这里只是告知了模型如何做，最好也能够给模型做个**示范**：

> 用户输入到 GPT-3：
> -     请把以下中文翻译成英文：
> -     苹果 => apple;
> -     掘金还挺不错的 => Juejin is rather good；
> -     你觉得 JioNLP 是个好用的工具吗？=>

  


其中苹果翻译成 apple，是一个示范样例，用于让模型感知该输出什么。只给提示，不给示范叫做 zero-shot，给一个范例叫做 one-shot，给多个范例叫做 few-shot。这很符合人们的直觉。

  


> 例如，语文老师给全班布置了一篇作文，我们第一时间拿到作文题后的反应是什么？一定是找范文，而且要参考不止一篇范文。这就是 few-shot 的含义。

  


GPT3 中的例子如下图所示，其中展示了输入文本教模型学习法语的例子。

  


![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50c786ec37784804aa75cdad053f0d9d~tplv-k3u1fbpfcp-zoom-1.image)

范例给几个就行了，不能再给多了！一方面，如前所述，获取标注数据代价很大，一般来说不可能有那么多针对特定任务的标注数据；另一方面，给多了实际上就又成上图中右侧的 finetune 模式了，我们不能回到监督学习的老路上去。

  


## prompt 学习

在 NLP 学术界，还有一个 prompt 研究方向，它使用人类编写的简短文本片段（prompt）来指导模型生成文本。如果说 few-shot 方法是**亲身示范**，那么 prompt 就是**口头命令**。

举一个例子，你一看就明白：

> 用户输入到 GPT-3（**有 prompt** 的情况）：
> -     **请把以下中文翻译成英文：**
> -     苹果 => apple;
> -     掘金还挺不错的 => Juejin is rather good；
> -     你觉得 JioNLP 是个好用的工具吗？=>
>
> 用户输入到 GPT-3（无 **prompt** 的情况）：
> -     苹果 => apple;
> -     掘金还挺不错的 => Juejin is rather good；
> -     你觉得 JioNLP 是个好用的工具吗？=>

  


实际上，prompt 就是对任务做自然语言描述，不加 prompt 就是直接给范例。从本质上看，**in-context learning 和 prompt 完全就是一回事**。只不过描述的语境，具体的操作稍有不同。

  


## In-context learning 的效果评价

这种引导学习的方式，在小模型上效果还是十分捉急的，但它却在超大模型上展示了惊人的效果：只需要给出一个或几个示范样例，模型就能照猫画虎地给出正确答案。注意：必须是**超大模型**（即参数数量非常多）才可以。

  


![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6c3dd8ed058c43f7b9fe77f97bd81439~tplv-k3u1fbpfcp-zoom-1.image)

在上图中：

-   few-shot 的效果要好于 zero-shot，毕竟模型有学习的参考对象（就像有了范文，写作文质量会好一些）。
-   有 prompt 要比没有 prompt 效果好，毕竟模型知道了要学习的任务本身，这都符合人的直觉。
-   蓝色的曲线代表 1750 亿参数的模型效果，而在 13 亿参数的绿色曲线上，模型效果则十分捉急。它毫无疑问地证明了**高级的人工智能必然依赖超大规模的神经网络**。

  


> 当前，ChatGPT 十分火爆，互联网上有很多的**类 ChatGPT 模型**出来，它们很多都是在利用小规模神经网络实现：
>
> -   开源 XX 模型，可将 ChatGPT 部署在 12G 大小的显卡上
> -   开源 YY 模型，只需一张 3090 显卡，人手一个 ChatGPT 的平替
>
> 你也可以根据我们的课程讲解自行判断，这些本地化部署的小规模神经网络到底是货真价实？还是哗众取宠？

  


## In-context learning 的局限性

上文所讲的例子都有一个特点，给模型的输入不自然，比较死板：

> 用户输入到 GPT-3：
> -     请把以下中文翻译成英文：
> -     苹果 => apple;
> -     你觉得 JioNLP 是个好用的工具吗？=>
>
> 而实际上，我们自然语言中很少用到 `=>`这种符号，用户告诉模型的输入是纯自然语言：
> -   翻译一下，中译英：你觉得 JioNLP 是个好用的工具吗？
> -   你觉得 JioNLP 是个好用的工具吗？给我翻译成英文
> -   ...

  


自然语言对事物的描述是非常灵活的，单纯 in-context learning 对如此灵活的用户指令还不够适应。

此外，小样本学习得到的 GPT 模型在做问答任务时，有时会编造虚构事实、产生有误的、带有歧视偏见的回答。

总而言之，这些回答与用户的需求不契合。这就是接下来一节要介绍的，由强化学习 RLHF 来完成。

  


# 总结

-   NLP 模型的训练阶段总共经历了4个阶段：纯监督学习、预训练+finetune、小样本学习、RLHF。
-   所有 GPT 模型都采用了基于大规模数据的语言模型预训练。
-   GPT3 使用了 in-context learning 的学习方式，扩增语言模型的学习能力。
-   In-context learning 对人类自然语言指令的响应仍有不足。